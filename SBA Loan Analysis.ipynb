{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e460ade",
   "metadata": {},
   "source": [
    "### INTRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5bef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38adbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_more(df,normalize_ind=False, weight_column=None, skip_columns=[], dropna=True) :\n",
    "    var = [] ; l = [] ; t = []; unq =[]; min_l = []; max_l = [];\n",
    "    assert isinstance(skip_columns, list), \"Argument skip_columns should be list\"\n",
    "    if weight_column is not None:\n",
    "        if weight_column not in list(df.columns):\n",
    "            raise AssertionError('weight_column is not a valid column name in the input DataFrame')\n",
    "      \n",
    "    for x in df:\n",
    "        if x in skip_columns:\n",
    "            pass\n",
    "        else:\n",
    "            var.append( x )\n",
    "            uniq_counts = len(pd.value_counts(df[x],dropna=dropna))\n",
    "            uniq_counts = len(pd.value_counts(df[x], dropna=dropna)[pd.value_counts(df[x],dropna=dropna)>0])\n",
    "            l.append(uniq_counts)\n",
    "            t.append( df[ x ].dtypes )\n",
    "            min_l.append(df[x].apply(str).str.len().min())\n",
    "            max_l.append(df[x].apply(str).str.len().max())\n",
    "            if weight_column is not None and x not in skip_columns:\n",
    "                df2 = df.groupby(x).agg({weight_column: 'sum'}).sort_values(weight_column, ascending=False)\n",
    "                df2['authtrans_vts_cnt']=((df2[weight_column])/df2[weight_column].sum()).round(2)\n",
    "                unq.append(df2.head(n=100).to_dict()[weight_column])\n",
    "            else:\n",
    "                df_cat_d = df[x].value_counts(normalize=normalize_ind,dropna=dropna).round(decimals=2)\n",
    "                df_cat_d = df_cat_d[df_cat_d>0]\n",
    "                #unq.append(df[x].value_counts().iloc[0:100].to_dict())\n",
    "                unq.append(df_cat_d.iloc[0:100].to_dict())\n",
    "            \n",
    "    levels = pd.DataFrame( { 'A_Variable' : var , 'Levels' : l , 'Datatype' : t ,\n",
    "                             'Min Length' : min_l, 'Max Length': max_l, 'Level_Values' : unq} )\n",
    "    return levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/onkar/Downloads/SBA_archive/SBAnational.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a722bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb897040",
   "metadata": {},
   "source": [
    "## 1. DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb8249",
   "metadata": {},
   "source": [
    "##### According to the dataset documentation provided :\n",
    "\n",
    "There are a number of variables that consistently emerge as indicators of risk that could explain the variation of loan default rates. Seven variables, along with some exploratory analysis, are discussed below including Location (State), Industry, Gross Disbursement, New vs Established Business, Loans Backed by Real Estate, Economic Recession, and SBA's Guaranteed Portion of Approved Loan.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(keep='first')   # delete duplicates, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5200cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['MIS_Status','RevLineCr','LowDoc','Bank','BankState',\n",
    "                  'Name','City','State','NewExist','DisbursementDate'],inplace=True)\n",
    "\n",
    "# dropped all but ChgOffDate null values as we do not really need that column in particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bae2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIS_Status'].value_counts()   # we want to see how many loans have defaulted, indicated by status as 'CHGOFF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(axis=1,columns=['ChgOffDate'])   # dropped ChgOffDate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8687bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_more(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33be67",
   "metadata": {},
   "source": [
    "\n",
    "##### # We see here that some records of DisbursementGross, BalanceGross, ChgOffPrinGr, GrAppv & SBA_Appv are being read as objects (strings) instead of numbers (integers)\n",
    "\n",
    "\n",
    "##### > Some things we note or want to do are: \n",
    "\n",
    "     1. drop records that dont have Y or N values of LowDoc\n",
    "     2. some ApprovalFY records are also not integers\n",
    "     3. drop NewExist records that have value 0\n",
    "     4. convert ApprovalDate and DisbursementDate columns to datetime values\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf55535",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_mess=[\"DisbursementGross\", \"BalanceGross\", \"ChgOffPrinGr\", \"GrAppv\", \"SBA_Appv\"]\n",
    "df[cols_mess]=df[cols_mess].astype(str).replace(\"[']\",\"\",regex=True)   # drop single quotation marks\n",
    "df[cols_mess]=df[cols_mess].astype(str).replace(\"[,]\",\"\",regex=True)   # drop commas\n",
    "df[cols_mess]=df[cols_mess].astype(str).replace(\"[$]\",\"\",regex=True)   # drop $ sign\n",
    "\n",
    "df[cols_mess]=df[cols_mess].astype(float)   # convert the cols_mess columns to float data type\n",
    "\n",
    "df[cols_mess].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ApprovalFY\"].unique()   # note that some years are within quotation marks and one is followed by an A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ApprovalFY\"]=df[\"ApprovalFY\"].astype(str).replace(\"'\",\"\",regex=True)\n",
    "df[\"ApprovalFY\"]=df[\"ApprovalFY\"].astype(str).replace(\"A\",\"\",regex=True)\n",
    "\n",
    "df[\"ApprovalFY\"]=df[\"ApprovalFY\"].astype(int)   # convert ApprovalFY dtype from object to int\n",
    "\n",
    "df[\"ApprovalFY\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd920557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['LowDoc']=='Y') | (df['LowDoc']=='N')]   # dropped all but Y/N records in LowDoc\n",
    "df['LowDoc'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['NewExist']==1) | (df['NewExist']==2)]   # dropped unspecified values (0) in NewExist\n",
    "df[\"NewExist\"]=df[\"NewExist\"].astype(int)\n",
    "\n",
    "print(df[\"NewExist\"].isna().sum())\n",
    "print(df[\"NewExist\"].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0406e6",
   "metadata": {},
   "source": [
    "##### # Some of the fields that are considered flags already but aren't necessarily in a useable format right now. \n",
    "These include the NewExist, RevLineCr, LowDoc, and MIS_Status fields; some of which need fixing. \n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIS_Status'].where(~(df[\"MIS_Status\"]=='P I F'), other=0, inplace=True)    # replace PIF as 0\n",
    "df['MIS_Status'].where(~(df[\"MIS_Status\"]=='CHGOFF'), other=1, inplace=True)   # replace CHGOFF as 1\n",
    "\n",
    "df[\"MIS_Status\"]=df[\"MIS_Status\"].astype(int)\n",
    "df[\"MIS_Status\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LowDoc'].where(~(df[\"LowDoc\"]=='N'), other=0, inplace=True)   # replace N as 0\n",
    "df['LowDoc'].where(~(df[\"LowDoc\"]=='Y'), other=1, inplace=True)   # replace Y as 1\n",
    "\n",
    "df[\"LowDoc\"]=df[\"LowDoc\"].astype(int)\n",
    "df[\"LowDoc\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95452de8",
   "metadata": {},
   "source": [
    "##### > We can now work to clean RevLineCr which has a large records (especially of 0) having unneccessary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48343e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"RevLineCr\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71dcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['RevLineCr']=='Y') | (df['RevLineCr']=='N') | (df['RevLineCr']=='0')]\n",
    "df[\"RevLineCr\"].value_counts()   # dropped all garbage value of RevLineCr and left 0 as unknown factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RevLineCr'].where(~(df[\"RevLineCr\"]=='N'), other=0, inplace=True)   # replace N as 0\n",
    "df['RevLineCr'].where(~(df[\"RevLineCr\"]=='Y'), other=1, inplace=True)   # replace Y as 1\n",
    "df['RevLineCr'].where(~(df[\"RevLineCr\"]==0), other=-1, inplace=True)    # replace 0 as -1\n",
    "\n",
    "df[\"RevLineCr\"]=df[\"RevLineCr\"].astype(int)\n",
    "df[\"RevLineCr\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005af1f",
   "metadata": {},
   "source": [
    "##### > Convert ApprovalDate and DisbursementDate columns to datetime values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d631cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "df[['ApprovalDate','DisbursementDate']]=df[['ApprovalDate','DisbursementDate']].apply(\n",
    "                                            lambda x: pd.to_datetime(x,errors = 'coerce'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e698bf3",
   "metadata": {},
   "source": [
    "##### > Check if all missing values have been dealt with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61422c7",
   "metadata": {},
   "source": [
    "##### > Check whether all columns we manipulated have the right datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00422e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd01d3ad",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b300a9",
   "metadata": {},
   "source": [
    "##### > We have franchise codes which are unneccessary, we need only know whether a business is a franchise or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bacff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['FranchiseCode'] <= 1),'Franchise'] = 0\n",
    "df.loc[(df['FranchiseCode'] > 1),'Franchise'] = 1\n",
    "\n",
    "df[\"Franchise\"]=df[\"Franchise\"].astype(int)      # Convert datatype of Franchise to int\n",
    "df.drop(columns=\"FranchiseCode\", inplace=True)   # Drop FranchiseCode since its of no particular use to us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05d2fc",
   "metadata": {},
   "source": [
    "##### > Create a new column named \"Industry\" with the industry the NAICS code represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industry'] = df['NAICS'].astype('str').apply(lambda x: x[:2])   # Select only first 2 numbers of NAICS code\n",
    "\n",
    "df['Industry'] = df['Industry'].map({\n",
    "    '11': 'Ag/For/Fish/Hunt',\n",
    "    '21': 'Min/Quar/Oil_Gas_ext',\n",
    "    '22': 'Utilities',\n",
    "    '23': 'Construction',\n",
    "    '31': 'Manufacturing',\n",
    "    '32': 'Manufacturing',\n",
    "    '33': 'Manufacturing',\n",
    "    '42': 'Wholesale_trade',\n",
    "    '44': 'Retail_trade',\n",
    "    '45': 'Retail_trade',\n",
    "    '48': 'Trans/Ware',\n",
    "    '49': 'Trans/Ware',\n",
    "    '51': 'Information',\n",
    "    '52': 'Finance/Insurance',\n",
    "    '53': 'RE/Rental/Lease',\n",
    "    '54': 'Prof/Science/Tech',\n",
    "    '55': 'Mgmt_comp',\n",
    "    '56': 'Admin_sup/Waste_Mgmt_Rem',\n",
    "    '61': 'Educational',\n",
    "    '62': 'Healthcare/Social_assist',\n",
    "    '71': 'Arts/Entertain/Rec',\n",
    "    '72': 'Accom/Food_serv',\n",
    "    '81': 'Other_no_pub',\n",
    "    '92': 'Public_Admin'\n",
    "})   # Map the approprate industry to each record based on the first two digits of the NAICS code\n",
    "\n",
    "df.dropna(subset=['Industry'], inplace=True)   # Remove records where Industry is NaN (NAICS code was 0)\n",
    "df.drop(columns=\"NAICS\", inplace=True)         # Drop NAICS since its of no particular use to us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f7d04",
   "metadata": {},
   "source": [
    "##### > Feature showing the guaranteed amount as percentage of the gross loan amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SBA_ApvPct']=round(df['SBA_Appv']/df['GrAppv'],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b51b1e",
   "metadata": {},
   "source": [
    "##### > Feature for loans backed by Real Estate (loans with a term of at least 20 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54fccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RealEstate']=np.where(df['Term']>=240,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3725fb",
   "metadata": {},
   "source": [
    "##### > Feature for loans active during the Great Recession (2007-2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ceece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DisbursementFY']=df['DisbursementDate'].map(lambda x: x.year)   # find year of disbursement\n",
    "\n",
    "df['GreatRecession']=np.where(((2007 <= df['DisbursementFY']) & (df['DisbursementFY'] <= 2009)) | \n",
    "                        ((df['DisbursementFY'] < 2007) & (df['DisbursementFY'] + (df['Term']/12) >= 2007)), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472762a",
   "metadata": {},
   "source": [
    "##### > Select only records with a disbursement year through 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['DisbursementFY']<=2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790341a",
   "metadata": {},
   "source": [
    "##### # According to dataset document: \n",
    "An emphasis is placed on the default rates of loans with a disbursement date through 2010. We chose this time period for two reasons. We want to account for variation due to the Great Recession (December 2007 to June 2009); so loans disbursed before, during, and after this time frame are needed. Secondly, we restrict the time frame to loans by  excluding those disbursed after 2010 due to the fact the term of a loan is frequently 5 or more years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87276a",
   "metadata": {},
   "source": [
    "##### > We will now drop columns that are not of use to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['LoanNr_ChkDgt','Name','City','Zip','Bank','BankState', 'NoEmp', 'CreateJob', 'RetainedJob',\n",
    "                 'ChgOffPrinGr','ApprovalDate','DisbursementDate'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0ad69",
   "metadata": {},
   "source": [
    "##### # The columns I dropped and the reason why is stated below -\n",
    "\n",
    "LoanNr_ChkDgt, Name, City, Zip, Bank, BankState, NoEmp, CreateJob, RetainedJob - provide no value to the actual analysis\n",
    "\n",
    "ApprovalDate -  unneccessary as we already have ApprovalFY\n",
    "\n",
    "DisbursementDate - replaced by DisbursementFY\n",
    "\n",
    "ChgOffPrinGr - amount not of use to us, we already have MIS_Status\n",
    "\n",
    "We dont drop State as we want to check the effect of demograpphy on the loan default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f87f1",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection & Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af98663",
   "metadata": {},
   "source": [
    "##### > Let's check the Gross Disbursement column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a224f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "for i in df:\n",
    "    fig = px.histogram(df, x=i, color_discrete_sequence=px.colors.qualitative.Antique, text_auto=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71bedda",
   "metadata": {},
   "source": [
    "##### # Looking at these histograms, here are some attributes we see that need attention -\n",
    "\n",
    "1. Term should not be more than 300 months (25 years)\n",
    "2. DisbursementGross should not be more than 2.0M\n",
    "3. GrAppv & SBA_Appv should not be more than 0.5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Term'] <= 300]\n",
    "df = df[df['DisbursementGross'] <= 2000000]\n",
    "df = df[df['GrAppv'] <= 500000]\n",
    "df = df[df['SBA_Appv'] <= 500000]\n",
    "\n",
    "print(\"Max Term:\",df['Term'].max())\n",
    "print(\"Max Disbursement Gross:\",df['DisbursementGross'].max())\n",
    "print(\"Max Gross Approved:\",df['GrAppv'].max())\n",
    "print(\"Max SBA Approved :\",df['SBA_Appv'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb223eb",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f3357",
   "metadata": {},
   "source": [
    "##### # INSIGHTS:\n",
    "\n",
    " 1. Mean of SBA Approved Percentage is about 60% of total loan amount needed\n",
    " 2. Most loans were disbursed between 1997-2008 (approval FY)\n",
    " 3. Loans backed by Real Estate are about 10% of total loans\n",
    " 4. An average of 75% loans were active during the Great Recession (2007-2009)\n",
    " 5. Only about 5% of loans were given to Franchises, while 27.78% loans were given to new businesses\n",
    " 6. More than 90% of loans were given to businesses in an Urban area\n",
    " 7. Average term of loan was around 8 years with a std. dev. of 5 years\n",
    " 8. More than 10% of loans sanctioned were LowDoc program (1 page loan doc for amount <150k)\n",
    " 9. Average loan Gross Disbursement was around 120k with under 75% of them being around 160k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.histplot(df.GrAppv, color=\"green\", kde=False)\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Approved ammount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76b91c",
   "metadata": {},
   "source": [
    "##### # Observation - This plot shows the right skewedness of GrAppv, so we fix the skewness using a log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfebd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GrAppv']=np.log(df['GrAppv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c137453",
   "metadata": {},
   "source": [
    "## 5. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='MIS_Status', y='Term', data=df)\n",
    "plt.title('Average Term vs MIS_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='MIS_Status', y='DisbursementGross', data=df)\n",
    "plt.title('Average DisbursementGross vs MIS_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='MIS_Status', y='GrAppv', data=df)\n",
    "plt.title('Average GrAppv vs MIS_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='MIS_Status', y='SBA_Appv', data=df)\n",
    "plt.title('Average SBA_Appv vs MIS_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total/Average disbursed loan amount by industry\n",
    "# Create a groupby object on Industry for use in visualization\n",
    "industry_group = df.groupby(['Industry'])\n",
    "\n",
    "# Data frames based on groupby by Industry looking at aggregate and average values\n",
    "df_industrySum = industry_group.sum().sort_values('DisbursementGross', ascending=False)\n",
    "df_industryAve = industry_group.mean().sort_values('DisbursementGross', ascending=False)\n",
    "\n",
    "# Establish figure for placing bar charts side-by-side\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "\n",
    "# Add subplots to figure to build 1x2 grid and specify position of each subplot\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "# ______ Bar chart 1 = Gross SBA Loan Disbursement by Industry ______\n",
    "\n",
    "ax1.bar(df_industrySum.index, df_industrySum['DisbursementGross'] / 1000000000)\n",
    "ax1.set_xticklabels(df_industrySum.index, rotation=30, horizontalalignment='right', fontsize=10)\n",
    "\n",
    "ax1.set_title('Gross SBA Loan Disbursement by Industry from 1984-2010', fontsize=15)\n",
    "ax1.set_xlabel('Industry')\n",
    "ax1.set_ylabel('Gross Loan Disbursement (Billions)')\n",
    "\n",
    "# ______ Bar chart 2 = Average SBA Loan Disbursement by Industry ______\n",
    "\n",
    "ax2.bar(df_industryAve.index, df_industryAve['DisbursementGross'])\n",
    "ax2.set_xticklabels(df_industryAve.index, rotation=30, horizontalalignment='right', fontsize=10)\n",
    "\n",
    "ax2.set_title('Average SBA Loan Disbursement by Industry from 1984-2010', fontsize=15)\n",
    "ax2.set_xlabel('Industry')\n",
    "ax2.set_ylabel('Average Loan Disbursement')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a44d0e",
   "metadata": {},
   "source": [
    "#### > Loans categorized by Industry and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf775e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Paid in full and defaulted loans\n",
    "\n",
    "fig3 = plt.figure(figsize=(15, 10))\n",
    "\n",
    "ax1a = plt.subplot(2, 1, 1)\n",
    "ax2a = plt.subplot(2, 1, 2)\n",
    "\n",
    "# Function for creating stacked bar charts grouped by desired column\n",
    "# df = original data frame, col = x-axis grouping, stack_col = column to show stacked values\n",
    "# Essentially acts as a stacked histogram when stack_col is a flag variable\n",
    "\n",
    "def stacked_setup(df, col, axes, stack_col='MIS_Status'):\n",
    "    data = df.groupby([col, stack_col])[col].count().unstack(stack_col)\n",
    "    data.fillna(0)\n",
    "\n",
    "    axes.bar(data.index, data[1], label='Default')\n",
    "    axes.bar(data.index, data[0], bottom=data[1], label='Paid in full')\n",
    "\n",
    "\n",
    "# ______ Number of Paid in full and defaulted loans by INDUSTRY ______\n",
    "\n",
    "stacked_setup(df=df, col='Industry', axes=ax1a)\n",
    "ax1a.set_xticklabels(df.groupby(['Industry', 'MIS_Status'])['Industry'].count().unstack('MIS_Status').index,\n",
    "                     rotation=35, horizontalalignment='right', fontsize=10)\n",
    "\n",
    "ax1a.set_title('Number of PIF/Defaulted Loans by Industry from 1984-2010', fontsize=15)\n",
    "ax1a.set_xlabel('Industry')\n",
    "ax1a.set_ylabel('Number of PIF/Defaulted Loans')\n",
    "ax1a.legend()\n",
    "\n",
    "# ______ Number of Paid in full and defaulted loans by STATE ______\n",
    "\n",
    "stacked_setup(df=df, col='State', axes=ax2a)\n",
    "\n",
    "ax2a.set_title('Number of PIF/Defaulted Loans by State from 1984-2010', fontsize=15)\n",
    "ax2a.set_xlabel('State')\n",
    "ax2a.set_ylabel('Number of PIF/Defaulted Loans')\n",
    "ax2a.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2c1bc",
   "metadata": {},
   "source": [
    "##### > Default percentage by Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f190f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_ind = df.groupby(['Industry', 'MIS_Status'])['Industry'].count().unstack('MIS_Status')\n",
    "def_ind['Def_Percent'] = def_ind[1]/(def_ind[1] + def_ind[0])\n",
    "def_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93c6432",
   "metadata": {},
   "source": [
    "##### > Check Default percentage by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a2578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def_state = df.groupby(['State', 'MIS_Status'])['State'].count().unstack('MIS_Status')\n",
    "def_state['Def_Percent'] = def_state[1]/(def_state[1] + def_state[0])\n",
    "def_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2d32f",
   "metadata": {},
   "source": [
    "##### > Loans paid in full and Defaulted loans by DisbursementFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ax4 = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "stack_data = df.groupby(['DisbursementFY', 'MIS_Status'])['DisbursementFY'].count().unstack('MIS_Status')\n",
    "x = stack_data.index\n",
    "y = [stack_data[1], stack_data[0]]\n",
    "\n",
    "ax4.stackplot(x, y, labels=['Default', 'Paid in full'])\n",
    "ax4.set_title('Number of PIF/Defaulted Loans by State from 1984-2010', fontsize=15)\n",
    "ax4.set_xlabel('Disbursement Year')\n",
    "ax4.set_ylabel('Number of PIF/Defaulted Loans')\n",
    "ax4.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# We use a stacked area chart here since it's time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abcb620",
   "metadata": {},
   "source": [
    "##### # It can be clearly seen that most loans have defaulted in the time-period leading up to the Great Reccession 2008 Also note how the number of loans have icreased until the Great Reccesion and then sharply decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ea714",
   "metadata": {},
   "source": [
    "##### > Loans backed by Real Estate v/s Loans during the Great Recession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ced09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____ Paid in full and defaulted loans backed by Real Estate ______\n",
    "\n",
    "fig5 = plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax1b = fig5.add_subplot(1, 2, 1)\n",
    "ax2b = fig5.add_subplot(1, 2, 2)\n",
    "\n",
    "stacked_setup(df=df, col='RealEstate', axes=ax1b)\n",
    "ax1b.set_xticks(df.groupby(['RealEstate', 'MIS_Status'])['RealEstate'].count().unstack('MIS_Status').index)\n",
    "ax1b.set_xticklabels(labels=['No', 'Yes'])\n",
    "\n",
    "ax1b.set_title('Number of PIF/Defaulted Loans backed by Real Estate from 1984-2010', fontsize=15)\n",
    "ax1b.set_xlabel('Loan Backed by Real Estate')\n",
    "ax1b.set_ylabel('Number of Loans')\n",
    "ax1b.legend()\n",
    "\n",
    "# ______ Paid in full and defaulted loans active during the Great Recession ______\n",
    "\n",
    "stacked_setup(df=df, col='GreatRecession', axes=ax2b)\n",
    "ax2b.set_xticks(df.groupby(['GreatRecession', 'MIS_Status'])['GreatRecession'].count().unstack('MIS_Status').index)\n",
    "ax2b.set_xticklabels(labels=['No', 'Yes'])\n",
    "\n",
    "ax2b.set_title('Number of PIF/Defaulted Loans Active during the Great Recession from 1984-2010', fontsize=15)\n",
    "ax2b.set_xlabel('Loan Active during Great Recession')\n",
    "ax2b.set_ylabel('Number of Loans')\n",
    "ax2b.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad298b74",
   "metadata": {},
   "source": [
    "##### # Surprisingly, the volume of loans backed by real estate was much less than those not backed by real estate however the default rate is also much less for loans backed by real estate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b531e4",
   "metadata": {},
   "source": [
    "##### > Default percentage for loans backed by Real Estate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36498600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_re = df.groupby(['RealEstate', 'MIS_Status'])['RealEstate'].count().unstack('MIS_Status')\n",
    "def_re['Def_Percent'] = def_re[1]/(def_re[1] + def_re[0])\n",
    "def_re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927ce57",
   "metadata": {},
   "source": [
    "##### > Default percentage for loans active during the Great Recession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_gr = df.groupby(['GreatRecession', 'MIS_Status'])['GreatRecession'].count().unstack('MIS_Status')\n",
    "def_gr['Def_Percent'] = def_gr[1]/(def_gr[1] + def_gr[0])\n",
    "def_gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261d5f8",
   "metadata": {},
   "source": [
    "## 6. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77224431",
   "metadata": {},
   "source": [
    "##### > Dummy encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c5801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23351eb3",
   "metadata": {},
   "source": [
    "##### > Split data into train and test sets + label target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f909ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.MIS_Status\n",
    "X = df.drop(['MIS_Status'], axis=1)\n",
    "\n",
    "# Scale the feature values prior to modeling\n",
    "scale = StandardScaler()\n",
    "X_scaled = scale.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe76f54",
   "metadata": {},
   "source": [
    "#### > LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21763e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=0)   # Initialize model\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_logpred = log_reg.predict(X_val)   # Train model and make predictions\n",
    "\n",
    "print(classification_report(y_val, y_logpred, digits=3))   # Print results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218635e",
   "metadata": {},
   "source": [
    "##### # Observation -\n",
    "\n",
    "We can see here that with the Logistic Regression model, we have a surprising accuracy of 85%, and the F1-score of 60% for defaulted loans doesn't seem promising at all. The precision suggests that the model is correct 70% of the time when the loan defaults, and the recall suggests that the model identifies only a 50% of defaulted loans correctly. That means that every one of two loans that defaulted were incorrectly classified as loans that would be paid in full, which is horribly bad business for any loan providing entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2d68f",
   "metadata": {},
   "source": [
    "#### > DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6866ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "pred = dtc.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, pred))\n",
    "print(confusion_matrix(y_val, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd710ad8",
   "metadata": {},
   "source": [
    "##### # note how our scores have increased vastly as compared to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4b155",
   "metadata": {},
   "source": [
    "#### > XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f445f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier(random_state=0)\n",
    "\n",
    "xgboost.fit(X_train, y_train)\n",
    "y_xgbpred = xgboost.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_xgbpred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39aa97",
   "metadata": {},
   "source": [
    "##### # scores have increased slightly more than Decision Trees, we can still tune hyperparameters if we want to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae36bb8",
   "metadata": {},
   "source": [
    "#### > List the importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1c074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, importance in sorted(zip(X.columns, xgboost.feature_importances_)):\n",
    "    print(name, \"=\", round(importance*100,3), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df725e9b",
   "metadata": {},
   "source": [
    "##### # Observations -\n",
    "\n",
    "Top 5 features : \n",
    "    Term (17%)\n",
    "    --> ApprovalFY (6%)\n",
    "    --> SBA_ApvPct (3.4%)\n",
    "    --> State_CA (3.3%)\n",
    "    --> State_FL (2.8%)\n",
    "\n",
    "\n",
    "Top 3 industries : \n",
    "    Healthcare (2.2%)\n",
    "    --> Rental/Lease (1.1%) \n",
    "    --> Retail_Trade (1.1%)\n",
    "\n",
    "RealEstate has 0 effect on loan default, understandably since people taking such a risk have a pretty good plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c761d98",
   "metadata": {},
   "source": [
    "##### > Let's see if reducing the number of features used to the most important ones would have a positive impact on the model performance, since the current model has a high level of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188eb8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline   # Build pipeline for feature selection and modeling \n",
    "from sklearn.feature_selection import SelectKBest   # SelectKBest defaults to top 10 features\n",
    "\n",
    "xgb_featimp = XGBClassifier(random_state=0)\n",
    "pipe = Pipeline(steps=[ ('feature_selection', SelectKBest()), ('model', xgb_featimp) ])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_featimppred = pipe.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_featimppred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ca8bd",
   "metadata": {},
   "source": [
    "##### # Reducing the number of features, and thereby dimensionality of the data, didn't affect the results too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382628e",
   "metadata": {},
   "source": [
    "##### > List the importance of each feature for the lower dimensionality dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, importance in sorted(zip(X.columns, xgb_featimp.feature_importances_)):\n",
    "    print(name, \"=\", round(importance*100,3), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22e9ed",
   "metadata": {},
   "source": [
    "##### # Observation -\n",
    "\n",
    "Top 5 features are now Term (50%), ApprovalFY (16%), BalanceGross (10%), SBA_Appv (5.5%), NewExist (4.5%)\n",
    "\n",
    "Note how ApprovalFY and Term are the only initial features on this new Top 5 list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f865d6",
   "metadata": {},
   "source": [
    "### CONCLUSION\n",
    "\n",
    "##### According to the analysis, the factor that contributed the most to whether or not a loan defaulted is the loan's Term length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
